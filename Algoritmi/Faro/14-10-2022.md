>Bisogna riuscire a mitigare il più possibile e "**ottenere**" $O(n+k)$ quindi risolvere il problema della complessità del counting-sort

# RADIX-SORT
>Questo algoritmo è usato in mainframe con schede perforat

Esso funziona in maniera **controintuitiva**.
- Usa una **sottoprocedura** mediante un algoritmo di ordinamento stabile(counting-sort). Se l'algoritmo della sottoprocedura non è stabile, allora il radix-sort non opera in maniera corretta.

Esempio: Uso array verticale A:
![[Pasted image 20221016120243.png|150]]
Si nota che alcuni numeri sono un po' più lunghi di altri e questo non è un problema visto che per ogni numero è rappresentato con lo stesso numero di bit.
- Aggiungo allora lo 0 per rappresentare lo stesso numero di cifre.
![[Pasted image 20221016120458.png|150]]
Il numero di cifre del numero è $c$
- Ordina gli elementi usando come chiave una cifra di un numero e quindi viene considerata la i-esima cifra. Si ordinano i numeri solo rispetto ad una cifra, quindi una colonna, come se gli altri non ci fossero.
  - Si parte da quella meno significativa e via via verso quella più significativa, cioè da destra verso sinistra. 
  - Alla fine si ottiene un ordinamento finito.

considero la stabilità dell'algoritmo e quindi il più piccolo è in alto
- Ordino rispetto alla prima colonna a destra e ottengo:
![[Pasted image 20221016120739.png|150]]

- Adesso mi riferisco alla seconda colonna. vedo che già sono ordinati(quindi non ricopio l'ordinamento sopra.)

- Mi riferisco alla terza colonna (da destra) e ottengo:
![[Pasted image 20221016120937.png|150]]

- Adesso mi riferisco 4 colonna e ottengo
![[Pasted image 20221016121101.png|150]]
E così ottiene l'array ordinato.
Il counting sort viene ripetuto c volte

- $c*(n+k)$ --> k può essere considerata costante quindi:
- $c*O(n+b)$ b = numero da 0 a 9)-->c può essere considerato costante. Mai più grande di un tot, quindi viene considerato piccolo.
- quindi algoritmo è veramente $O(n)$ quindi lineare a dispetto di piccoli fattori di proporzionalità trascurabili. Non si ha il problema della pesantezza della memoria.

>b=base del numero, se in base 10 o base 2 ecc
>c=numero di cifre

# TABELLE HASH (capitolo 11)
Complessità costante, si fanno tante ricerche. (come un dizionario).
Si ha un insieme S di chiavi da voler rappresentare con una struttura dati contenuto in un insieme universale U

7 6 5 2 in S
8 1 13 11 in U ma non in S

Uso una tabella T che ha un certo numero di chiavi m
>0 1 2 3 4 5 6 7 8 9 //indici
>      2 3    5 6 7

**indirizzamento diretto** = le chiavi vengono direttamente indirizzate nella posizione che indica l'indice, quindi (vedi su)

si hann 3 operazioni in O(1) 
```
insert(T,k)
	T[k]=k
delete(T,k)
	T[k] = null
search(T,k)
	return T[k] = k)
```
La tabella si può implementare con bit 0 e 1 per vedere se c'è un numero oppure no OPPURE un contatore che conta quante volte un valore k è presente nell'insieme di chiavi S

Problematiche: per numeri infinitamente grandi non si può allocare T.
Abbandono l'indirizzamento diretto e uso tabelle hash

Hashing: è una funzione h. 
- Prendo un 5 in S, passo da h e h mi dice dove mettere il 5, in una posizione definita da h.
- Prendo il 5, chiedo a h dove metterlo in T, e lo metto in una posizione rispettiva dettata da h
Ottengo
>0 1 2 3 4 5 6 7 8 9 //indici
>   3    7 2          5 6

h:U->{0,1....m-1}

quindi 
```
insert(T,k)
	T[h(k)]=k
delete(T,k)
	T[h(k)] = null
search(T,k)
	return T[h(k)] = k)
```
Per elementi grandissimi, h mi dice dove inserire l'elemento e non c'è più il problema di spazio.
Il valore che produce h è univoco

U>>m (U molto più grande di m)
Esistono sicuramente k1 e k2 di U diverse fra loro per le quali h(k1) = h(k2) e questo evento si chiama COLLISIONE DI DUE CHIAVI (collisione nella stessa cella della tabella). Come si risolve? Con un array di liste concatenate
Quindi ogni elemento va nella lista presente in quella posizione T[i] quindi si ha T[List[i]]
In caso di collisione, l'elemento che collide si mette in testa alla lista della corrispettiva posizione
\alpha = n /m è fattore di carico dove n=elementni ed m=posizioni e dice quanto la tabella è piena
Se il fattore di carico =0 allora la tabella 
L'ideale è progettare al tabella affinche alpha=1 cosi non ci sono sprechi di spazio e collisioni

Tabella hash che risolve il problema di collisione per concatenazione
tabella hash concatenata 

allora
```
insert(T,k) O(1) -> inserisco in testa
	LIST-INSERT(T(h(k)),k) //puntatore testa lista,elemento da inserire)
delete(T,k) O(1) -> con una DLList
	LIST-DELETE (T[h(k)])
search(T,k) -> O(n)
	return LIST-SEARCH (T[h(k)],k)
```
Cancellare è O(1) perchè ho un puntatore a quell'elemento
Search caso pessimo O(n) perchè tutti gli elementi collidono fra loro
Il caso medio della ricerca può essere considerato costante per certi versi
- ci si pone in condizione ideale e h lavora alla perfezione(**hashing uniforme semplice**): quando inserisco una chiave k, la probabilità che h(k)=i è uguale per tutte le posizioni, cioè la probabilità che esca un numero 0 <= i \<m
- Quindi la probabilità per tutte è 1/m, uguale per tutte le i
- Suppongo di avere 2 chiavi k_i e k_j app. a U. Probabilità che $h(k_i)=h(k_j)$
- La prima può andare dove vuole ma mi interessa che la seconda va a finire dove la prima con probabilità 1/m, la prima non entra in gioco
- Questo evento lo chiamo $X_{ij} = Pr = {h(k_i)=h(k_j)}$
**Ricerca senza successo**
$S=\{k1,k2,k3...kn\}$
E[|T[i]] = n/m = alpha
quanti elementi guarda? alpha, in media
il numero di passi che l'algoritmo fa prima di accorgersi che un elemento non è presente.
Quindi la ricerca senza successo ha O(alpha+1). In media è costante quindi perchè la lista è lunga in media alpha

Costo di ricerca con successo
$S=\{k1,k2,k3...kn\}$
La ricerca con successo piu breve rispetto alla precedente, ma quanto piu breve?

1<= k_i \<=n
prima di k_i ho inserito k1 k2 k3 .... ki-1 e dopo k ho k1 k2 k3 .... kn
Suppongo di essere in posizione T[h(k)] -> NODO -> NODO -> NODO -> k(i)

Tutti gli elemento dopo k(i) non rientrano nel costo della ricerca, quindi guardo solo quello che vengono prima. Visto che l'insert viene in testa allora devo considerare gli elementi che vengono inseriti DOPO k_i. gli elementi inseriti DOPO vengono sempre inseriti in testa e quindi, rispetto a k_i, guardando la lista, si trovano più a sinistra.

quindi T[h(k_i)] -> NODO -> NODO -> NODO -> NODO -> k_i

La lunghezza della lista che si trova prima di k_i (sinistra di k_i, NODO NODO NODO NODO) quindi è sommatoria da j=i+1 A n di 1/m

Lavoro che serve per cercare la chiave $k_i$: tempo-> 1 (hash) + sommatoria da j=i+1 A n di 1/m
Voglio la media

(sommatoria per i=1 A n di (1+sommatoria per j=i+1 A n di 1/m)) /n 
COMPLESSITA' = T(n) = O ((sommatoria per i=1 A n di (1+sommatoria per j=i+1 A n di 1/m)) /n )

1/n\*(sommatoria per i=1 A n di (1+sommatoria per j=i+1 A n di 1/m))
posso portare 1/m fuori dalla sommatoria e ottengo
- linearità della sommatoria sommatoria di (x+y) = sommatoria x + sommatoria y
- la prima fa 1. 
- sposto fuori la m
- applico linearita
- la prima vale n^2
- la seconda vale (n(n+1)/2)
- ottengo -> 1 + (n/nm) - (n/2nm) = 1+ (alpha/2)-(alpha/2n) tempo medio per accedere ad un elemento con successo
- se alpha vicino 1 allora ho sicuramente un tempo costante

## Funzioni hash di divisione e di moltiplicazione

h: U->[0,1...m-1] è quello che vorrei ottenere
il metodo piu easy è quello della divisione.
- k app. U, allora h(k)=k mod m compreso fra 0 e m-1 SEMPRE (meccanismo per realizzare funzione hash ma bisogna stare attenti ai valori di m: se m=2^p allora ho problemi. Shift a destra di p posizioni perchè divido per potenze di 2). Prendo il resto della divisione, che se k era intero diventa con la virgola.
- significa che se k dipende solo da p  bit della chiave.
- se k2 è rappresentata con lo stesso numero di bit di k allora ci sono sempre i p bit da considerare. Quindi c'è molta più probabilità di collisioni fra 2 chiavi
