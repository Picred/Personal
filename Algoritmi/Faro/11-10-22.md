>$Capitolo \space 8$
# <font color ="red">ALBERO DI DECISIONE</font>
Valutiamo un algoritmo di ordinamento che **non opera per confronto** ma ed è il miglior algoritmo di ordinamento (dal 1954).

>Un algoritmo per confronti arriva al massimo a $\Omega (nlogn)$ ed è matematicamente dimostrato che non ci sono complessità migliori.

Dato un array $A=\{a,b,c\}$ e lo vogliamo ordinare. Per farlo uso un albero di decisione. 
- Ad ogni nodo ci poniamo una domanda scritta all'interno del nodo;
- Quindi si confrontano 2 elementi generici.
![[Pasted image 20221016103934.png]]

- Guardando l'albero ho dei rami più corti e più lunghi.
Il caso più semplice (**caso migliore**) ha $n-1$ confronti, ovvero l'altezza dell'albero nel caso migliore è $n-1$
Nel **caso** **peggiore** si finisce nei rami più lunghi. I confronti e il numero di foglie è $n!$ cioè tutte le possibili permutazioni di n elementi. 

>Quindi  $2^h\geq\#foglie \geq n!$
Quindi $n! \leq 2^h$ --> $h\geq log(n!)$

>Esiste **approssimazione di Stirling** se $n \geq n_0$ che dice:
>$log(n!) =~ n logn$
>Quindi $h \geq log(n!) \geq nlogn$
>da cui $h= \Omega (nlogn)$

Quindi nel caso pessimo si impiegano $nlogn$ passi 

# <font color ="red">COUNTING-SORT</font>
Questo algoritmo di ordinamento ordina n elementi **senza fare confronti** e la complessità abbatte il muro di $nlogn$ perchè non opera per confronti e abbatte il limite inferiore di qualsiasi altro algoritmo che opera per confronti.

Dato un array A:![[Pasted image 20221016105227.png]]
Distinguiamo elementi con la stessa chiave quindi otteniamo:
![[Pasted image 20221016105300.png]]

 I **possibili output** possono essere
 1.Ordinamento rispetto alla priorità è rispetto all'ordine di arrivo ![[Pasted image 20221016105415.png]]
 2. Ordinamento in base alla priorità ma NON rispetto all'ordine di arrivo![[Pasted image 20221016105513.png]]
Date queste due differenze si distingue un algoritmo stabile da un non stabile:
- Algoritmo **STABILE** = **mantiene l'ordine relativo** degli elementi con la stessa chiave (cioè se nell'array ho 2A e poi 2B, nel risultato avrò i 2 nell'ordine 2A e 2B);
- Algoritmo **NON SBABILE** = ci sono violazioni della stabilità dell'algoritmo, ovvero gli elementi **NON appaiono nel risultato nello stesso ordine dell'array di partenza.**

>Il merge-sort è stabile perchè nel merge, se nei 2 array ho uno stesso elemento si predilige l'elemento nell'array a sinistra (array *"L"*).
>Nel quick-sort invece è più difficile implementarlo in maniera stabile.
>Insertion-sort e Selection-sort sono semplici da implementare in maniera stabile

## <font color ="blu
		 e">Passi counting-sort</font>
![[Pasted image 20221016105227.png]]
1. Creo un array **C** con 6 elementi che rappresentano gli **elementi distinti** dell'array iniziale, e ottengo: ![[Pasted image 20221016110209.png]]

C ci dice **quanti elementi ci sono in A**, cioè conta quante volte un elemento è presente in A.
Inizialmente C(i) sono tutti 0, e man mano conta le copie degli elementi.
>Quante volte C(i) è presente in A? Oppure "*i è presente C(i) volte in A*"

Si crea un altro array B che sarà l'array ordinato in output
![[Pasted image 20221016110550.png]]
Quindi non si **modifica la struttura iniziale** ma si **crea un altro array** per rispondere a query temporanee senza modificare A, lasciandolo intatto. 

Generalmente l'ideale è **restituire** un output in un **array addizionale**, quindi B, per poter *permettere all'utente di utilizzare nuovamente l'array di partenza*.

Indico con $k=max(A)$ che è il numero più alto che si presenta in A
La **complessità** di tale algoritmo è $O(n)+O(k) = O(n+k)$

### <font color ="blue">Tutti gli elementi sono indicizzabili nell'array C?</font>
- Ogni tipo elemento è indicizzabile nell'array A. Questo vale anche per ogni struttura dati che si può considerare come sequenze di bit $01$. Quindi, con le opportune operazioni anch'essa si può indicizzare con un numero.
- I numeri decimali possono essere indicizzati in C moltiplicando tutti gli elementi di A per 100 (per esempio) ottenendo cosi numeri interi.

## <font color ="blue">Pseudocodice counting-sort</font>
``` cpp
counting-sort(A,n)
	k<-max(A,n); --> O(n)//una funzione 
	C<- new array(n);
	for i = 1 to k --> O(k)
		C[i]=0; //inizializzo a 0 tutti gli elementi di C
		
	for i =1 to n O(n)
		C[A[i]]=C[A[i]]+1 //indicizzo C

	B<-new array(n)
	j<-0 //posizioni in B
	// i scandisce C
 
	for i = 1 to k --> O(n)
		for h = 1 to C[i]
			B[j]=i; //indice che mi interessa che è l'elemento di A
			j++;
			
```
$Esempio: Dato \space l'array:$
1000 1002 1002 1001 1002 1000 1003 1004 posso fare max-min e invece di allocare 1000 posizioni e usare solo la parte finale dell'array allora uso solo 5 posizioni. 
A livello di codice si traduce:
``` cpp
counting-sort(A,n)
	m,M<-find-min-max(A,n)//una funzione 
	C<- new array(M-m+1);
	for i = 1 to M-m+1
		C[i]=0; //inizializzo a 0 tutti gli elementi di C
		
	for i =1 to n 
		C[A[i]-m+1]=C[A[i]-m+1]+1 //indicizzo C

	B<-new array(m)
	j<-0 //posizioni in B
	// i scandisce C
 
	for i = 1 to M-m+1 
		for h = 1 to C[i]
			B[j]=i+m-1; //indice che mi interessa che è l'elemento di A
			j++;
			
```
>Un esempio pratico è l'ordinamento di studenti per numero di matricola
>Se la variabilità fra gli elementi è bassa allora l'algoritmo è applicabile

- Questo algoritmo non è stabile perchè nemmeno vengono copiati gli elementi A i in C i ma sono degli elementi creati totalmente da 0. 
- Si ricreano nuovi elementi dal nulla quindi non si può parlare di stabilità. Le chiavi sono di C sono le stesse di A in ordine. 
- Quindi C è una **simulazione** di A di come esso dovrebbe essere se fosse ordinato. 
- Quindi C(i) contiene nuovi elementi.
>Se l'elemento A(i) avesse dei **valori satellite**, oltre ai numeri, cioè nome cognome e altre **informazioni aggiuntive**, non potrei nemmeno risalire ad essi dall'array ricevuto in output, ovvero da C.

# <font color ="red">COUNTING-SORT: VERSIONE MIGLIORE</font>
In questo caso, l'array in output B conterrà elementi di A copiati e incollati.

Prima di copiare gli elementi su B, faccio un'altra passata su C, **sommando a $i$ l'elemento** $i-1$. Dato l'array in figura A, rivedendo i passi si ha:
![[Pasted image 20221016105227.png]]
1. Creo un array **C** con 6 elementi che rappresentano gli **elementi distinti** dell'array iniziale, e ottengo: ![[Pasted image 20221016110209.png]]
2. Sommo i+(i-1)![[Pasted image 20221016112503.png]]
3. Il nuovo array **C** mi dice semplicemente: "**Quanti elementi $\leq$ di i**?" "C(i) elementi più piccoli di i" 
	1. 3 elementi piu piccoli di 2
	2. 5 elementi piu piccoli di 3
	3. ecc..
4. in posizione i=6 ho C(i)=3, quindi so dove deve finire tale elemento in B
5. Creo l'array da restituire in output **B**, dove B(i)=C(i)
6. Ogni volta che utilizzo C(i) lo decremento. Questa operazione garantisce la stabilità, oltre al fatto che devo scansionare l'array A da destra verso sinistra, quindi, semplicemente ottengo: ![[Pasted image 20221016110550.png]]
``` cpp
counting-sort(A,n) //stabile
	m,M<-find-min-max(A,n)//una funzione 
	C<- new array(M-m+1);
	for i = 1 to M-m+1
		C[i]=0; //inizializzo a 0 tutti gli elementi di C
		
	for i =1 to n 
		C[A[i]-m+1]=C[A[i]-m+1]+1 //indicizzo C

	for i = 2 to M-m+1 
		C[i]=C[i]+C[i-1]; //somma cumulativa su C[i] di quello che c'era prima

	//Scandisco A da dx a sx e copio gli elementi in base alle posizioni suggerite da C
	for i=n down to 1
		//B[C[A[i]]]=A[i]; //copia di elementi
		B[C[A[i]-m+1]]=A[i];
		//C[A[i]] = C[A[i]]-1;
		C[A[i]-m+1] = C[A[i]-m+1]-1
```


Esercizio:
A
>1 2 3 4 5 6 7 8 //indici
>5A 6A 6B 4A 5B 6C 3A 6D //elementi

min=3 max=6
C
>1 2 3 4  //indici shiftati
>1 1 2 4 //elementi di C dopo una scansione di A
>3 4 5 6 //indici originali

C
>1 2 3 4 //indici
>1 2 4 8

B vedendo A da dx a sx
>1 2 3 4 5 6 7 8
>3A 4A 5A 5B 6A 6B 6C 6D

*Si fanno 4 scansioni come se fossero 4 partition di quicksort.*
